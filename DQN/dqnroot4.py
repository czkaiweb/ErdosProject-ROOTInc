#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Tue Nov 13 21:17:11 2018

"""

import math, random
import sys

import numpy as np
from collections import deque
from utils import plotLearning

import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd
import torch.nn.functional as F
from IPython.display import clear_output
import matplotlib.pyplot as plt
from torch.autograd import Variable
from BidingEnvt import BidingEnvi
from DataPreprocessing.python.Data import *

sys.path.append('/home/aniket/Documents/erdos/ErdosProject-ROOTInc/Model/python/')

USE_CUDA = torch.cuda.is_available()
#Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)


class ReplayBuffer(object):
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        state      = np.expand_dims(state, 0)
        next_state = np.expand_dims(next_state, 0)

        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        #print "shape is: " + str(state)
        return np.concatenate(state), action, reward, np.concatenate(next_state), done

    def __len__(self):
        return len(self.buffer)


days = 1
dt = 5
now = 0
index = 0
#generate simulated impressions
maxctr = 0.02
basecost = 10
baseIntensity = 2

data = Data()
data.addFile("Root_Insurance_data.csv")
data.loadData()
df_data = data.getDataCopy()
env = BidingEnvi()
env.loadCustomerPool(df_data)

#action space size, this has nothing to do with simulation environment, it's a design parameter
#of the pacing algorithm
action_space_size = 20

dailyBudget = 50
budget = dailyBudget
ctrThres = 0
maxBid = 20
cpcGoal = 100


num_iters = 200
epsilon_start = 1.0
epsilon_final = 0.01
epsilon_decay = num_iters

epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)

class DQN(nn.Module):
    def __init__(self, num_states, num_actions):
        super(DQN, self).__init__()
        #print "state size: " + str(num_states)
        #print "action size: " + str(num_actions)
        self.layers = nn.Sequential(
            nn.Linear(num_states, 8),#128
            nn.ReLU(),
            #nn.Linear(32, 16),
            #nn.ReLU(),
            nn.Linear(8, num_actions)
        )

    def forward(self, x):
        return self.layers(x)

    def act(self, state, epsilon):
        if random.random() > epsilon:
            #combine state and parameter to get input to forward network
            augmentstate = Variable(torch.FloatTensor(np.concatenate([state,[env.initialBudget[0]]])).unsqueeze(0))
            #state  = Variable(torch.FloatTensor(state).unsqueeze(0))

            #Q_value corresponding to all actions
            q_value = self.forward(augmentstate)
            #action  = q_value.max(1)[1].data[0]
            action  = q_value.max(1)[1].item()
            #action  = q_value.min(1)[1].item()
            #print "generated by state"
        else:
            action = random.randrange(action_space_size)
            #print "generated by rnd"
        return action


def compute_td_loss(batch_size):
    #state and next state are already augmented
    state, action, reward, next_state, done = replay_buffer.sample(batch_size)
    #state      = Variable(torch.FloatTensor(np.float32(state)))
    state      = Variable(torch.FloatTensor(np.float32(state)))
    next_state = Variable(torch.FloatTensor(np.float32(next_state)))
    action     = Variable(torch.LongTensor(action))
    reward     = Variable(torch.FloatTensor(reward))
    done       = Variable(torch.FloatTensor(done))

    q_values      = model(state)
    next_q_values = model(next_state)

    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)
    next_q_value     = next_q_values.max(1)[0]
    expected_q_value = reward + gamma * next_q_value * (1 - done)

    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss



batch_size = 32
gamma = 0.99

losses = []
all_rewards = []
episode_reward = 0

BASELINE = False

param_size = 1 # only parameter is the total budget
model = DQN(2 + param_size, action_space_size)

if USE_CUDA:
    model = model.cuda()

if not BASELINE:

    optimizer = optim.Adam(model.parameters())

    replay_buffer = ReplayBuffer(20000)
    scores, eps_history, rembud =[],[],[]

    #train
    for iters in range(1,num_iters+1):
        print(iters)
        #change budget in dataconfig to randomized value
        state = env.reset()
        score = 0
        #todo: add for loop to cover all cases
        budgetval = 10000.0
        env.intialBudget = budgetval

        for i in range(2000):
            epsilon = epsilon_by_frame(i)
            action = model.act(state, epsilon)#discrete action
            #action = baseline.act(state)
            next_state, reward, done, _ = env.step(action)
            #print next_state
            #print reward
            #print done
            if state is not None and next_state is not None and done==False:
                augstate = np.concatenate([state,[env.initialBudget[0]]])
                augnext_state = np.concatenate([next_state,[env.initialBudget[0]]])
                replay_buffer.push(augstate, action, reward, augnext_state, done)

            state = next_state
            score += reward

            if done==True:
                break

            if len(replay_buffer) > batch_size:
                loss = compute_td_loss(batch_size)
                losses.append(loss.item())

        rembud.append(env.state[0])
        scores.append(score)
        eps_history.append(epsilon)
        avg_score = np.mean(scores)
        print('episode ', iters, 'score %.2f' % score,
                'average score %.2f' % avg_score,
                'Remaining budget %.2f' % env.state[0])







    print('Maximum reward=', max(scores),'at episode=', scores.index(max(scores)))
    x = [i+1 for i in range(num_iters)]
    filename = 'random.png'
    plotLearning(x, scores, eps_history, filename)

    # #test
    total_reward = 0
    actionrec = []
    spendrec = []
    timerec = []
    rewardhis = []
    #env.dailyBudget = 100
    state = env.reset()

    for i in range(2000):
            action = model.act(state, eps_history[scores.index(max(scores))])#discrete action
            #action = baseline.act(state)
            next_state, reward, done, _ = env.step(action)
            rewardhis.append(reward)
            actionrec.append(action)
            spendrec.append(next_state[0])
            #print next_state
            #print reward
            #print done
            if state is not None and next_state is not None and done==False:
                augstate = np.concatenate([state,[env.initialBudget[0]]])
                augnext_state = np.concatenate([next_state,[env.initialBudget[0]]])
                replay_buffer.push(augstate, action, reward, augnext_state, done)

            state = next_state
            total_reward += reward

            if done==True:
                break



    #plot pacing signals
    # y = actionrec
    # x = [i for i in range(len(actionrec))]
    # plt.plot(y)
    # plt.xlabel("time")
    # plt.ylabel("action")
    # plt.show()
    #plt.savefig("action-time.png")
    #plot spendings

    # y = spendrec
    # x = timerec
    # plt.plot(y)
    # plt.xlabel("time")
    # plt.ylabel("Remaining budget")
    # plt.show()
    # plt.savefig("budget-time.png")

    # print ("total reward is" + str(total_reward))


#debug, plot surface for sanity check

"""
#compare, naive bidding & pacing
total_reward = 0
actionrec = []
spendrec = []
timerec = []

dataconfig = Dataconfig(1,maxctr,basecost,datafile,baseIntensity)
env.state.dailyBudget = 100
state = env.reset(dataconfig)
print "data generated" + str(len(env.data))
debugrec = []
for idx in range(288):
    action = baseline.act(state, env,debugrec)
    actionrec.append(action)
    next_state, reward, done, time,remainBudget = env.pacingStep(action)
    timerec.append(time)
    spendrec.append(remainBudget)
    state = next_state
    total_reward += reward

#plot pacing signals
y = actionrec
x = [i for i in range(len(actionrec))]
plt.plot(y)
plt.show()
#plot spendings

y = spendrec
x = timerec
plt.plot(y)
plt.show()

"""

"""
#plot debug signals
print "plot of S " + str(len(debugrec))

y = [debugrec[i][0] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()

print "plot of remaining budget ratio"
y = [debugrec[i][1] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()

print "plot of remaining budget"
y = [debugrec[i][2] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()
"""
"""
print "plot of remaining budget ratio"
y = [debugrec[i][1] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()

print "plot of ref signal"
y = [debugrec[i][4] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()
"""
